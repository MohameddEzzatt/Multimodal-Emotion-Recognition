High-Accuracy Multimodal Emotion Detection from VideoThis repository contains a complete end-to-end system for Multimodal Emotion Detection, designed to accurately classify human emotions from video clips by analyzing both audio and visual data streams.The project implements an advanced pipeline that extracts deep features from facial expressions and acoustic properties from speech. These features are then intelligently combined using a novel Multi-View Fusion model. Finally, a powerful Stacking Ensemble Classifier is trained on the fused data to achieve high-performance emotion recognition.üåü Key Featuresüß† Multimodal Analysis: Leverages both audio and video channels to create a more robust and accurate understanding of emotional expressions.üñºÔ∏è Advanced Visual Feature Extraction: Utilizes the VGGFace (ResNet-50) model to extract powerful deep features from faces, enhanced with an Attention Pooling mechanism to focus on the most expressive frames in a video.üéµ Rich Audio Feature Extraction: Employs Librosa to extract a comprehensive set of acoustic features, including MFCCs, Spectral Centroid, Zero-Crossing Rate, and more.üîó Intelligent Feature Fusion: Implements a sophisticated Multi-View Fusion strategy that combines features using Concatenation, Cross-Attention, and a Gated mechanism to create a powerful, unified representation.‚öôÔ∏è High-Performance Modeling: Achieves state-of-the-art results using a Stacking Classifier that ensembles SVM, RandomForest, and XGBoost models for superior predictive power.üî¨ Project PipelineThe entire process is broken down into four distinct, sequential stages.Stage 1: Data PreprocessingThe initial step involves processing the raw video files from the RAVDESS dataset to separate the audio and visual components.Frame Extraction: Each video is sampled at a rate of 3 frames per second. These frames are saved into a dedicated folder for each video, preparing them for visual analysis.Audio Extraction: The audio track from each video clip is extracted and saved as a .wav file, ready for acoustic feature extraction.Stage 2: Feature ExtractionIn this stage, we extract meaningful features from both the visual and acoustic data.Audio Features: The Librosa library is used to analyze each .wav file. A set of 45 distinct acoustic features (MFCCs, ZCR, RMS, etc.) are extracted and saved to ravdess_features.csv.Visual Features:Each frame from Stage 1 is passed through the pre-trained VGGFace (ResNet-50) model to generate a deep feature vector.An Attention Pooling layer is applied to the sequence of frame features for each video. This layer intelligently assigns weights to each frame, allowing the model to focus on the most emotionally significant moments and produce a single, context-rich feature vector for the entire video.The final visual features are saved to video_features_vggface_attention.csv.Stage 3: Multi-View Feature FusionThis is the core stage where information from the two modalities is combined.Dimensionality Reduction: The high-dimensional visual features are first passed through PCA (Principal Component Analysis) to reduce their dimensionality to 300 components, making the model more efficient without losing significant information.Feature Scaling: Both audio and visual features are normalized using StandardScaler.Fusion Model: A Keras/TensorFlow model performs the fusion using three parallel techniques:Concatenation: A simple merging of the audio and video feature vectors.Cross-Attention: A MultiHeadAttention layer allows features from one modality to attend to features from the other, capturing cross-modal dependencies.Gated Fusion: A gating mechanism learns to control the contribution of each modality to the final fused vector.The outputs of these three techniques are concatenated to form the final, powerful feature vector, which is then saved to fusion_features.csv.Stage 4: Modeling and TrainingThe final stage involves training a classifier on the fused features.Data Splitting: The fused dataset is split into training, validation, and test sets.Model Training: A StackingClassifier is trained. This ensemble model uses SVC, RandomForestClassifier, and XGBClassifier as base learners and a final SVC as a meta-learner to combine their predictions.Evaluation: The model's performance is evaluated on the unseen test set to measure its accuracy and other key metrics.Model Saving: All trained components‚Äîincluding scalers, the PCA model, the fusion model, and the final stacking classifier‚Äîare saved to disk for future use.üìä ResultsThe model achieved excellent performance on the test set, demonstrating its effectiveness in recognizing a wide range of emotions.Final Test Accuracy: 93.06%Classification Report (Test Set)| Emotion | Precision | Recall | F1-Score || angry | 0.97 | 0.89 | 0.93 || calm | 0.93 | 0.97 | 0.95 || disgust | 0.95 | 0.95 | 0.95 || fearful | 0.95 | 0.90 | 0.92 || happy | 0.97 | 1.00 | 0.99 || neutral | 0.94 | 0.89 | 0.92 || sad | 0.87 | 0.89 | 0.88 || surprised | 0.88 | 0.92 | 0.90 |üöÄ How to RunFollow these steps to set up and run the project locally.Clone the Repositorygit clone [https://github.com/YOUR_USERNAME/YOUR_REPOSITORY.git](https://github.com/YOUR_USERNAME/YOUR_REPOSITORY.git)
cd YOUR_REPOSITORY

Set Up a Virtual Environment (Recommended)python -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`

Install Dependenciespip install -r requirements.txt

(Note: You will need to create a requirements.txt file containing all the necessary libraries.)Download the DatasetDownload the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) from this link.Extract the contents and place them in a folder named RAVDESS dataset at the root of the project directory.Run the Jupyter NotebookLaunch Jupyter Notebook or Jupyter Lab:jupyter notebook

Open the Final_main.ipynb file and run the cells in order.üõ†Ô∏è Technologies UsedCore Libraries: Python, NumPy, PandasMachine Learning: Scikit-learn, XGBoostDeep Learning: TensorFlow, KerasData Processing: OpenCV, MoviePy, LibrosaModel Persistence: Joblibüìú LicenseThis project is licensed under the MIT License. See the LICENSE file for more details.
