{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c9618aa879aadc",
   "metadata": {},
   "source": [
    "# Dataset Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08c24ce596b761",
   "metadata": {},
   "source": [
    "## Frame Generating From Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e742d53957cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "dataset_path = r\"D:\\fundamentals\\AI NTI\\Second Final Project\\RAVDESS dataset\"\n",
    "output_path = r\"D:\\fundamentals\\AI NTI\\Second Final Project\\frames_output\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "def extract_frames(video_path, output_folder, frames_per_sec=3):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 0:\n",
    "        print(f\"Can not read: {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_interval = int(fps / frames_per_sec)  \n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "    \n",
    "    video_folder = os.path.join(output_folder, video_name)\n",
    "    os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "    count = 0\n",
    "    frame_idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_idx % frame_interval == 0:\n",
    "            frame_filename = f\"{video_name}_frame{count}.jpg\"\n",
    "            frame_path = os.path.join(video_folder, frame_filename)\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            count += 1\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"{count} frames saved for {video_name}\")\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith((\".mp4\", \".avi\", \".mov\")):\n",
    "            video_path = os.path.join(root, file)\n",
    "            extract_frames(video_path, output_path, frames_per_sec=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331775755bcf1a6c",
   "metadata": {},
   "source": [
    " ## Audio Generating From Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d302898ef1def89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "import tqdm\n",
    "\n",
    "OUT_AUDIO_DIR = \"ravdess_wavs\"\n",
    "os.makedirs(OUT_AUDIO_DIR, exist_ok=True)\n",
    "\n",
    "def parse_label_from_filename(filename):\n",
    "    parts = os.path.basename(filename).split('.')[0].split('-')\n",
    "    emotion_code = parts[2]\n",
    "    emotion_map = {\n",
    "        '01':'neutral','02':'calm','03':'happy','04':'sad',\n",
    "        '05':'angry','06':'fearful','07':'disgust','08':'surprised'\n",
    "    }\n",
    "    return emotion_map.get(emotion_code, 'unknown')\n",
    "\n",
    "for root, _, files in os.walk(r\"RAVDESS dataset\"):\n",
    "    for fname in tqdm([f for f in files if f.endswith('.mp4')]):\n",
    "        video_path = os.path.join(root, fname)\n",
    "        base = os.path.splitext(fname)[0]\n",
    "\n",
    "        # 1) Extract audio\n",
    "        wav_out = os.path.join(OUT_AUDIO_DIR, base + \".wav\")\n",
    "        if not os.path.exists(wav_out):\n",
    "            clip = VideoFileClip(video_path)\n",
    "            clip.audio.write_audiofile(wav_out, verbose=False, logger=None)\n",
    "            clip.reader.close()\n",
    "            clip.audio.reader.close_proc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca488d319cb9fdfe",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c738c9733a1d778",
   "metadata": {},
   "source": [
    "## Audio Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset_path = r\"D:\\fundamentals\\AI NTI\\Second Final Project\\ravdess_wavs\"\n",
    "\n",
    "\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=22050)\n",
    "\n",
    "        # MFCCs\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
    "\n",
    "        # Zero Crossing Rate\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(y).T, axis=0)\n",
    "\n",
    "        # Spectral Centroid\n",
    "        centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr).T, axis=0)\n",
    "\n",
    "        # Spectral Bandwidth\n",
    "        bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr).T, axis=0)\n",
    "\n",
    "        # Spectral Rolloff\n",
    "        rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr).T, axis=0)\n",
    "\n",
    "        # Root Mean Square Energy\n",
    "        rms = np.mean(librosa.feature.rms(y=y).T, axis=0)\n",
    "\n",
    "        # all features\n",
    "        features = np.hstack([mfccs, zcr, centroid, bandwidth, rolloff, rms])\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ========== mapping ==========\n",
    "emotion_map = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "# ========== Apply Function==========\n",
    "all_features = []\n",
    "file_names = []\n",
    "labels = []\n",
    "video_ids = []\n",
    "\n",
    "for file in os.listdir(dataset_path):\n",
    "    if file.endswith(\".wav\") or file.endswith(\".mp3\"):\n",
    "        file_path = os.path.join(dataset_path, file)\n",
    "\n",
    "        # features extraction\n",
    "        features = extract_features(file_path)\n",
    "        if features is not None:\n",
    "            all_features.append(features)\n",
    "            file_names.append(file)\n",
    "\n",
    "            code = file.split(\"-\")[2]\n",
    "            label = emotion_map.get(code, \"unknown\")\n",
    "            labels.append(label)\n",
    "\n",
    "\n",
    "            video_id = os.path.splitext(file)[0]\n",
    "            video_ids.append(video_id)\n",
    "\n",
    "# ========== Save CSV ==========\n",
    "df = pd.DataFrame(all_features)\n",
    "df.insert(0, \"video_id\", video_ids)     \n",
    "df.insert(1, \"file_name\", file_names)   \n",
    "df.insert(2, \"label\", labels)           \n",
    "df.to_csv(\"ravdess_features.csv\", index=False)\n",
    "\n",
    "print(\"Feature extraction done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb88600a815487",
   "metadata": {},
   "source": [
    "## Frames Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9227bc209a0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import tensorflow as tf\n",
    "\n",
    "# VGGFace (ResNet50 backbone)\n",
    "model = VGGFace(model='resnet50', include_top=False, pooling='avg')\n",
    "\n",
    "\n",
    "frames_path = r\"D:\\fundamentals\\AI NTI\\Second Final Project\\frames_output\"\n",
    "\n",
    "def extract_image_features(img_path):\n",
    "    img = load_img(img_path, target_size=(224, 224))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array, version=2)  \n",
    "    features = model.predict(img_array, verbose=0)\n",
    "    return features.flatten()\n",
    "\n",
    "def attention_pooling(frame_feats):\n",
    "    \n",
    "    frame_feats = np.array(frame_feats)  # (T, D)\n",
    "    T, D = frame_feats.shape\n",
    "\n",
    "    \n",
    "    dense = Dense(1)\n",
    "    scores = dense(frame_feats)  # (T, 1)\n",
    "    scores = tf.nn.softmax(scores, axis=0).numpy()  \n",
    "\n",
    "    # weighted sum\n",
    "    video_feat = np.sum(frame_feats * scores, axis=0)  # (D,)\n",
    "    return video_feat\n",
    "\n",
    "\n",
    "video_features = []\n",
    "video_ids = []\n",
    "\n",
    "\n",
    "for video_folder in tqdm(os.listdir(frames_path)):\n",
    "    folder_path = os.path.join(frames_path, video_folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    frame_feats = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith((\".jpg\", \".png\")):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            feats = extract_image_features(img_path)\n",
    "            frame_feats.append(feats)\n",
    "\n",
    "    if len(frame_feats) > 0:\n",
    "        # attention pooling \n",
    "        video_feat = attention_pooling(frame_feats)\n",
    "        video_features.append(video_feat)\n",
    "\n",
    "        \n",
    "        video_ids.append(video_folder)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(video_features)\n",
    "df.insert(0, \"video_id\", video_ids)\n",
    "\n",
    "\n",
    "df.to_csv(\"video_features_vggface_attention.csv\", index=False)\n",
    "print(\"Feature extraction done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3dffc4b96b9a7",
   "metadata": {},
   "source": [
    "# Feature Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7349bffd34fd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T16:11:59.687997Z",
     "start_time": "2025-09-17T16:11:51.639953Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\main_tf_gpy\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-View Fusion Done! الملف fusion_features_multiview.csv اتعمل.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# ========= 1) =========\n",
    "audio_df = pd.read_csv(\"ravdess_features.csv\")\n",
    "video_df = pd.read_csv(\"video_features_vggface_attention.csv\")\n",
    "\n",
    "# ========= 2)  =========\n",
    "df = pd.merge(audio_df, video_df, on=\"video_id\")\n",
    "\n",
    "# ========= 3) =========\n",
    "audio_feats = df.iloc[:, 3:48].values  # audio features\n",
    "video_feats = df.iloc[:, 48:].values   # video features\n",
    "labels = df[\"label\"]\n",
    "video_ids = df[\"video_id\"]\n",
    "\n",
    "# ========= 4) Normalization =========\n",
    "scaler_audio = StandardScaler()\n",
    "scaler_video = StandardScaler()\n",
    "audio_scaled = scaler_audio.fit_transform(audio_feats)\n",
    "video_scaled = scaler_video.fit_transform(video_feats)\n",
    "\n",
    "# ========= 5) PCA  =========\n",
    "pca = PCA(n_components=300, random_state=42)\n",
    "video_reduced = pca.fit_transform(video_scaled)\n",
    "\n",
    "# ========= 6) Fusion Models =========\n",
    "\n",
    "# --- Inputs ---\n",
    "audio_in = Input(shape=(audio_scaled.shape[1],))\n",
    "video_in = Input(shape=(video_reduced.shape[1],))\n",
    "\n",
    "# --- (1) Concatenation Fusion ---\n",
    "fusion_concat = Concatenate()([audio_in, video_in])\n",
    "\n",
    "# --- (2) Cross Attention Fusion ---\n",
    "audio_proj = Dense(128, activation=\"relu\")(audio_in)\n",
    "video_proj = Dense(128, activation=\"relu\")(video_in)\n",
    "\n",
    "audio_seq = tf.expand_dims(audio_proj, axis=1)\n",
    "video_seq = tf.expand_dims(video_proj, axis=1)\n",
    "\n",
    "attn_audio = MultiHeadAttention(num_heads=4, key_dim=32)(audio_seq, video_seq)\n",
    "attn_video = MultiHeadAttention(num_heads=4, key_dim=32)(video_seq, audio_seq)\n",
    "\n",
    "attn_audio = tf.squeeze(attn_audio, axis=1)\n",
    "attn_video = tf.squeeze(attn_video, axis=1)\n",
    "\n",
    "fusion_attention = Concatenate()([attn_audio, attn_video])\n",
    "\n",
    "# --- (3) Gated Fusion ---\n",
    "gate_audio = Dense(1, activation=\"sigmoid\")(audio_in)\n",
    "gate_video = Dense(1, activation=\"sigmoid\")(video_in)\n",
    "\n",
    "gates = Concatenate()([gate_audio, gate_video])\n",
    "gates = tf.keras.layers.Softmax(axis=-1)(gates)\n",
    "\n",
    "alpha = gates[:, 0:1]\n",
    "beta = gates[:, 1:2]\n",
    "\n",
    "fusion_gated = Concatenate()([alpha * audio_in, beta * video_in])\n",
    "\n",
    "# --- (Final Multi-View Fusion) ---\n",
    "fusion_final = Concatenate()([fusion_concat, fusion_attention, fusion_gated])\n",
    "\n",
    "# ========= 7) features Extraction =========\n",
    "fusion_model = Model(inputs=[audio_in, video_in], outputs=fusion_final)\n",
    "fusion_features = fusion_model.predict([audio_scaled, video_reduced], verbose=0)\n",
    "\n",
    "# ========= 8) DataFrame =========\n",
    "fusion_df = pd.DataFrame(fusion_features)\n",
    "fusion_df.insert(0, \"video_id\", video_ids)\n",
    "fusion_df.insert(1, \"label\", labels)\n",
    "\n",
    "fusion_df.to_csv(\"fusion_features.csv\", index=False)\n",
    "print(\"Multi-View Fusion Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6107e9dced532a",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77fda652c77575",
   "metadata": {},
   "source": [
    "## Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e964394cf72559ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T16:55:10.095898Z",
     "start_time": "2025-09-17T16:51:23.774268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation Accuracy: 93.97%\n",
      "✅ Test Accuracy: 93.06%\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.97      0.89      0.93        38\n",
      "        calm       0.93      0.97      0.95        38\n",
      "     disgust       0.95      0.95      0.95        38\n",
      "     fearful       0.95      0.90      0.92        39\n",
      "       happy       0.97      1.00      0.99        39\n",
      "     neutral       0.94      0.89      0.92        19\n",
      "         sad       0.87      0.89      0.88        38\n",
      "   surprised       0.88      0.92      0.90        39\n",
      "\n",
      "    accuracy                           0.93       288\n",
      "   macro avg       0.93      0.93      0.93       288\n",
      "weighted avg       0.93      0.93      0.93       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ========= 1) =========\n",
    "fusion_df = pd.read_csv(\"fusion_features.csv\")\n",
    "\n",
    "# ========= 2) =========\n",
    "labels = fusion_df[\"label\"]\n",
    "video_ids = fusion_df[\"video_id\"]  # optional\n",
    "features = fusion_df.drop(columns=[\"video_id\", \"label\"])\n",
    "\n",
    "# ========= 3) Label Encoding =========\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# ========= 4) Train-Test-Validation Split =========\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    features, labels_encoded, test_size=0.2, random_state=42, stratify=labels_encoded\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.1, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# ========= 5) Scaling =========\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ========= 6) Base Classifiers =========\n",
    "svm_clf = SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42)\n",
    "rf_clf = RandomForestClassifier(n_estimators=300, max_depth=None, random_state=42)\n",
    "xgb_clf = XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "                        subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "\n",
    "# ========= 7) Stacking Classifier =========    \n",
    "stack_clf = StackingClassifier(\n",
    "    estimators=[('svm', svm_clf), ('rf', rf_clf), ('xgb', xgb_clf)],\n",
    "    final_estimator=SVC(kernel='linear', probability=True, random_state=42),\n",
    "    stack_method=\"predict_proba\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ========= 8) Training =========\n",
    "stack_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ========= 9) Evaluation =========\n",
    "y_val_pred = stack_clf.predict(X_val_scaled)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "y_test_pred = stack_clf.predict(X_test_scaled)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "y_val_pred_labels = label_encoder.inverse_transform(y_val_pred)\n",
    "y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
    "y_val_labels = label_encoder.inverse_transform(y_val)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "print(f\"✅ Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "print(f\"✅ Test Accuracy: {test_acc*100:.2f}%\\n\")\n",
    "\n",
    "print(\"Classification Report (Test):\")\n",
    "print(classification_report(y_test_labels, y_test_pred_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e7113342f4891",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61111011fe4ce286",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T16:24:45.385477Z",
     "start_time": "2025-09-17T16:24:45.162815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation Accuracy: 93.97%\n",
      "✅ Test Accuracy: 93.06%\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.97      0.89      0.93        38\n",
      "        calm       0.93      0.97      0.95        38\n",
      "     disgust       0.95      0.95      0.95        38\n",
      "     fearful       0.95      0.90      0.92        39\n",
      "       happy       0.97      1.00      0.99        39\n",
      "     neutral       0.94      0.89      0.92        19\n",
      "         sad       0.87      0.89      0.88        38\n",
      "   surprised       0.88      0.92      0.90        39\n",
      "\n",
      "    accuracy                           0.93       288\n",
      "   macro avg       0.93      0.93      0.93       288\n",
      "weighted avg       0.93      0.93      0.93       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = stack_clf.predict(X_val_scaled)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "y_test_pred = stack_clf.predict(X_test_scaled)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"✅ Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "print(f\"✅ Test Accuracy: {test_acc*100:.2f}%\\n\")\n",
    "\n",
    "print(\"Classification Report (Test):\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae1585c1872fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb4a384e0b27bd85",
   "metadata": {},
   "source": [
    "## Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4275d03ffb2d182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T16:59:18.466084Z",
     "start_time": "2025-09-17T16:59:18.249599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "✅ All Fusion + Training models & scalers & encoders saved!\n"
     ]
    }
   ],
   "source": [
    "# ========== 7) Save Everything ==========\n",
    "import joblib\n",
    "import os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Fusion step\n",
    "joblib.dump(scaler_audio, \"models/scaler_audio.joblib\")\n",
    "joblib.dump(scaler_video, \"models/scaler_video.joblib\")\n",
    "joblib.dump(pca, \"models/pca_video.joblib\")\n",
    "fusion_model.save(\"models/fusion_model.h5\")\n",
    "\n",
    "# Training step\n",
    "joblib.dump(scaler, \"models/fusion_scaler.joblib\")\n",
    "joblib.dump(svm_clf, \"models/svm_model.joblib\")\n",
    "joblib.dump(rf_clf, \"models/rf_model.joblib\")\n",
    "joblib.dump(xgb_clf, \"models/xgb_model.joblib\")\n",
    "joblib.dump(stack_clf, \"models/stacking_model.joblib\")\n",
    "joblib.dump(label_encoder, \"models/label_encoder.joblib\")\n",
    "\n",
    "print(\"All Fusion + Training models & scalers & encoders saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87774b3584782a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
